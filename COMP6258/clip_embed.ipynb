{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frames_path = '/Users/aahd/Library/CloudStorage/OneDrive-UniversityofSouthampton/year_4/Deep Learning/cw/DL_MindVideo/wen_2018/video_test_256_3hz.npy'\n",
    "segment_ids_path = '/Users/aahd/Library/CloudStorage/OneDrive-UniversityofSouthampton/year_4/Deep Learning/cw/DL_MindVideo/wen_2018/test_seg_id_3hz.npy'\n",
    "text_ids_path = '/Users/aahd/Library/CloudStorage/OneDrive-UniversityofSouthampton/year_4/Deep Learning/cw/DL_MindVideo/wen_2018/text_test_256_3hz.npy'\n",
    "fmri_ids_path = '/Users/aahd/Library/CloudStorage/OneDrive-UniversityofSouthampton/year_4/Deep Learning/cw/DL_MindVideo/wen_2018/fmri_test_subject1.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aahd/miniconda3/envs/mind-video/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def load_clip_model():\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    return model, processor\n",
    "\n",
    "def get_clip_embeddings(model, processor, image_tensor, text_caption):\n",
    "    inputs = processor(text=[text_caption], images=image_tensor, return_tensors=\"pt\", padding=True).to(device)\n",
    "    outputs = model(**inputs)\n",
    "    image_embedding = outputs.image_embeds.squeeze().detach()\n",
    "    text_embedding = outputs.text_embeds.squeeze().detach()\n",
    "    return image_embedding, text_embedding\n",
    "\n",
    "def process_embeddings(video_frames, text_ids, model, processor):\n",
    "    image_embeddings_list = []\n",
    "    text_embeddings_list = []\n",
    "    total_segments = video_frames.shape[0] * video_frames.shape[1]\n",
    "    progress_bar = tqdm(total=total_segments, desc=\"Processing Video and Text Pairs\")\n",
    "    \n",
    "    for i in range(video_frames.shape[0]):\n",
    "        for j in range(video_frames.shape[1]):\n",
    "            frame = video_frames[i, j, 0]  # Taking the first frame of each segment\n",
    "            text = \"Sample text\"  # Placeholder for actual text extraction logic\n",
    "            image_tensor = Image.fromarray(frame)\n",
    "            image_embedding, text_embedding = get_clip_embeddings(model, processor, image_tensor, text)\n",
    "            \n",
    "            image_embeddings_list.append(image_embedding)\n",
    "            text_embeddings_list.append(text_embedding)\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "    \n",
    "    progress_bar.close()\n",
    "\n",
    "    image_embeddings_tensor = torch.stack(image_embeddings_list).to(device)\n",
    "    text_embeddings_tensor = torch.stack(text_embeddings_list).to(device)\n",
    "    \n",
    "    return text_embeddings_tensor, image_embeddings_tensor\n",
    "\n",
    "def plot_similarity(similarity_matrix, title):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(similarity_matrix.cpu().detach().numpy(), cmap='viridis')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "video_frames = np.load(video_frames_path)\n",
    "text_ids = np.load(text_ids_path)\n",
    "segment_ids = np.load(segment_ids_path)\n",
    "\n",
    "# Load model and processor\n",
    "model, processor = load_clip_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Video and Text Pairs: 100%|██████████| 1200/1200 [02:30<00:00,  7.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Analysis \n",
    "# text_embed, video_embed = process_embeddings(video_frames, text_ids, model, processor, (image_projection_head, text_projection_head))\n",
    "text_embed, video_embed = process_embeddings(video_frames, text_ids, model, processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the text_embedding as the a pickle file\n",
    "with open('text_embedding.pkl', 'wb') as f:\n",
    "    pickle.dump(text_embed, f)\n",
    "    \n",
    "#save the video_embedding as the a pickle file\n",
    "with open('video_embedding.pkl', 'wb') as f:\n",
    "    pickle.dump(video_embed, f)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mind-video",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
